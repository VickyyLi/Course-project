{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Multi-label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name(s): Li Weijing\n",
    "\n",
    "Student Number(s): 19204246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import other useful packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Load the Yeast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.004168 -0.170975 -0.156748 -0.142151  0.058781  0.026851  0.197719   \n",
       "1 -0.103956  0.011879 -0.098986 -0.054501 -0.007970  0.049113 -0.030580   \n",
       "2  0.509949  0.401709  0.293799  0.087714  0.011686 -0.006411 -0.006255   \n",
       "3  0.119092  0.004412 -0.002262  0.072254  0.044512 -0.051467  0.074686   \n",
       "4  0.042037  0.007054 -0.069483  0.081015 -0.048207  0.089446 -0.004947   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.041850  0.066938 -0.056617  ...       0       0       1       1       0   \n",
       "1 -0.077933 -0.080529 -0.016267  ...       0       0       0       0       0   \n",
       "2  0.013646 -0.040666 -0.024447  ...       0       0       0       0       0   \n",
       "3 -0.007670  0.079438  0.062184  ...       0       0       0       0       0   \n",
       "4  0.064456 -0.133387  0.068878  ...       1       1       0       0       0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0        0        0        1        1        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        1        1        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from csv\n",
    "dataset = pd.read_csv('yeast.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "      <th>Class4</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class1  Class2  Class3  Class4  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0       0       0       0       0       0       0       1       1       0   \n",
       "1       0       0       1       1       0       0       0       0       0   \n",
       "2       0       1       1       0       0       0       0       0       0   \n",
       "3       0       0       1       1       0       0       0       0       0   \n",
       "4       0       0       1       1       1       1       0       0       0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0        0        0        1        1        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        1        1        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:,:-14]\n",
    "y = dataset.iloc[:,-14:]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(X,y,random_state=0,train_size = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement the Binary Relevance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class Binary_Relevance():\n",
    "    def __init__(self, BaseEstimator=\"decision tree\",undersampling = False,print_ = True):\n",
    "        \"\"\"Binary Relevance for multi-label classification\n",
    "        Use train_fit to train model, and use predict_model to predict\n",
    "        Parameters\n",
    "        ----------\n",
    "        BaseEstimator : 'decision tree' or 'logistic regression' or 'SVM'.\n",
    "        undersampling : True or False\n",
    "        print_ : print process or not\n",
    "        -------\n",
    "        \"\"\"\n",
    "        self.BaseEstimator = BaseEstimator\n",
    "        self.feature_num = 0\n",
    "        self.undersampling = undersampling\n",
    "        self.print_ = print_\n",
    "        \n",
    "    def train_fit(self,X,y):\n",
    "        if self.print_==True:\n",
    "            print(\"Use %s to train\"%self.BaseEstimator)\n",
    "        self.feature_num = len(y.iloc[0,:])\n",
    "        for i in range(0,self.feature_num):\n",
    "            if self.undersampling==False:\n",
    "                y_slice = y.iloc[:,i]\n",
    "                X_slice = X\n",
    "                #print(y_slice.value_counts())\n",
    "            elif self.undersampling==True:\n",
    "                X_slice,y_slice=RandomUnderSampler(random_state=0).fit_resample(X, y.iloc[:,i])\n",
    "                #print(y_slice.value_counts())\n",
    "            if self.BaseEstimator == 'SVM':\n",
    "                model = BaggingClassifier(base_estimator=SVC()).fit(X_slice,y_slice)\n",
    "                joblib.dump(model, \"model_store/task1_%s.m\"%(str(i)))\n",
    "            elif self.BaseEstimator == 'logistic regression':\n",
    "                model = BaggingClassifier(base_estimator=LogisticRegression()).fit(X_slice,y_slice)\n",
    "                joblib.dump(model, \"model_store/task1_%s.m\"%(str(i)))\n",
    "            elif self.BaseEstimator == 'decision tree':\n",
    "                model = RandomForestClassifier().fit(X_slice,y_slice)\n",
    "                joblib.dump(model, \"model_store/task1_%s.m\"%(str(i)))\n",
    "        return self\n",
    "    def predict_model(self,X):\n",
    "        y_output = pd.DataFrame()\n",
    "        for i in range(0,self.feature_num):\n",
    "            model = joblib.load(\"model_store/task1_%s.m\"%(str(i)))\n",
    "            y_ptr = pd.DataFrame(model.predict(X))\n",
    "            y_output = pd.concat([y_output,y_ptr],axis = 1)\n",
    "        if self.print_==True:\n",
    "            print(\"Successfully predict\")\n",
    "        return y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use decision tree to train\n",
      "Successfully predict\n",
      "Use logistic regression to train\n",
      "Successfully predict\n",
      "Use SVM to train\n",
      "Successfully predict\n",
      "         decision tree  logistic regression       SVM\n",
      "Class1        0.986226             0.986226  0.986226\n",
      "Class2        0.723140             0.710744  0.707989\n",
      "Class3        0.725895             0.716253  0.732782\n",
      "Class4        0.888430             0.888430  0.892562\n",
      "Class5        0.896694             0.896694  0.895317\n",
      "Class6        0.920110             0.920110  0.920110\n",
      "Class7        0.814050             0.807163  0.812672\n",
      "Class8        0.852617             0.841598  0.852617\n",
      "Class9        0.778237             0.761708  0.776860\n",
      "Class10       0.794766             0.754821  0.798898\n",
      "Class11       0.742424             0.742424  0.746556\n",
      "Class12       0.730028             0.684573  0.721763\n",
      "Class13       0.633609             0.611570  0.640496\n",
      "Class14       0.787879             0.783747  0.794766\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use Binary_Relevance() to train and predit with imbalanced training data, and\n",
    "show the accuracy of each feature of different BaseEstimator\n",
    "\"\"\"\n",
    "def binary_relevance_train_imbalanced():\n",
    "    base_estimator_list=['decision tree', 'logistic regression', 'SVM']\n",
    "    accuracy_list_for_imbalanced = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['Class'+str(i) for i in range(1,15)])\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        binary_ = Binary_Relevance(base_estimator_list[j])\n",
    "        binary_.train_fit(X_train,y_train)\n",
    "        y_pred = binary_.predict_model(X_test)\n",
    "        accuracy_n = []\n",
    "        for i in range(0,len(y_test.iloc[0,:])):\n",
    "            accuracy_ptr =metrics.accuracy_score(y_test.iloc[:,i],y_pred.iloc[:,i])\n",
    "            accuracy_n.append(accuracy_ptr)\n",
    "        accuracy_n.reverse()\n",
    "        accuracy_list_for_imbalanced.iloc[:,j]=accuracy_n\n",
    "    return accuracy_list_for_imbalanced\n",
    "accuracy_list_for_imbalanced = binary_relevance_train_imbalanced()\n",
    "print(accuracy_list_for_imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use decision tree to train\n",
      "Successfully predict\n",
      "Use logistic regression to train\n",
      "Successfully predict\n",
      "Use SVM to train\n",
      "Successfully predict\n",
      "         decision tree  logistic regression       SVM\n",
      "Class1        0.549587             0.466942  0.512397\n",
      "Class2        0.573003             0.557851  0.570248\n",
      "Class3        0.555096             0.567493  0.574380\n",
      "Class4        0.630854             0.630854  0.670799\n",
      "Class5        0.636364             0.646006  0.698347\n",
      "Class6        0.556474             0.545455  0.522039\n",
      "Class7        0.644628             0.611570  0.612948\n",
      "Class8        0.679063             0.603306  0.652893\n",
      "Class9        0.655647             0.625344  0.665289\n",
      "Class10       0.741047             0.673554  0.753444\n",
      "Class11       0.721763             0.683196  0.713499\n",
      "Class12       0.723140             0.684573  0.702479\n",
      "Class13       0.625344             0.595041  0.644628\n",
      "Class14       0.768595             0.734160  0.769972\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use Binary_Relevance() to train and predit with balanced training data, and \n",
    "show the accuracy of each feature of different BaseEstimator\n",
    "\"\"\"\n",
    "def binary_relevance_train_balanced():\n",
    "    base_estimator_list=['decision tree', 'logistic regression', 'SVM']\n",
    "    accuracy_list_for_under_sample = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['Class'+str(i) for i in range(1,15)])\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        binary_ = Binary_Relevance(base_estimator_list[j],True)\n",
    "        binary_.train_fit(X_train,y_train)\n",
    "        y_pred = binary_.predict_model(X_test)\n",
    "        accuracy_n = []\n",
    "        for i in range(0,len(y_test.iloc[0,:])):\n",
    "            accuracy_ptr =metrics.accuracy_score(y_test.iloc[:,i],y_pred.iloc[:,i])\n",
    "            accuracy_n.append(accuracy_ptr)\n",
    "        accuracy_n.reverse()\n",
    "        accuracy_list_for_under_sample.iloc[:,j]=accuracy_n\n",
    "    return accuracy_list_for_under_sample\n",
    "accuracy_list_for_under_sample = binary_relevance_train_balanced()\n",
    "print(accuracy_list_for_under_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating imbalanced data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:17<00:00, 19.76s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.69s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:59<00:00, 29.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating balanced data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:16<00:00,  7.62s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:52<00:00, 11.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss\n",
      "           decision tree logistic regression       SVM\n",
      "imbalanced      0.188791            0.199282  0.186407\n",
      "balanced        0.347453            0.365083  0.340572\n",
      "macro-averaged f-score\n",
      "           decision tree logistic regression       SVM\n",
      "imbalanced      0.365289            0.348632  0.383072\n",
      "balanced        0.466935            0.454504  0.469602\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate use Hamming loss and macro-averaged f-score with k-fold cross validation.\n",
    "\"\"\"\n",
    "base_estimator_list=['decision tree', 'logistic regression', 'SVM']   \n",
    "\n",
    "accuracy_list1 = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['imbalanced','balanced'])\n",
    "accuracy_list2 = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['imbalanced','balanced'])\n",
    "\n",
    "def evaluation(train_dataset,base_estimator_list,accuracy_list1,accuracy_list2,k_fold_num):\n",
    "    def random_split(full_list,k_fold_num1):\n",
    "        offset = int(len(full_list)/k_fold_num1)\n",
    "        full_list = full_list.sample(frac=1).reset_index(drop=True)\n",
    "        split_point = []\n",
    "        for i in range(0,k_fold_num1):\n",
    "            split_point.append(offset*i)\n",
    "        split_point.append(len(full_list))\n",
    "        return split_point\n",
    "    split_ptr = random_split(train_dataset,k_fold_num)\n",
    "    print(\"evaluating imbalanced data\")\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        accuracy_aver1,accuracy_aver2 = 0,0\n",
    "        for i in tqdm(range(0,k_fold_num)):\n",
    "            X_train = pd.concat([train_dataset.iloc[:split_ptr[i],:-14],train_dataset.iloc[split_ptr[i+1]:,:-14]],axis=0)\n",
    "            y_train = pd.concat([train_dataset.iloc[:split_ptr[i],-14:],train_dataset.iloc[split_ptr[i+1]:,-14:]],axis=0)\n",
    "            X_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],:-14]\n",
    "            y_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],-14:]\n",
    "            binary_ = Binary_Relevance(base_estimator_list[j],False,False)\n",
    "            binary_.train_fit(X_train,y_train)\n",
    "            y_pred = binary_.predict_model(X_valid)\n",
    "            accuracy_aver1 += metrics.hamming_loss(y_valid, y_pred)\n",
    "            accuracy_aver2 += metrics.f1_score(y_valid, y_pred, average='macro')\n",
    "        accuracy_list1.iloc[0,j]=accuracy_aver1/k_fold_num\n",
    "        accuracy_list2.iloc[0,j]=accuracy_aver2/k_fold_num  \n",
    "    print(\"evaluating balanced data\")\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        accuracy_aver1,accuracy_aver2 = 0,0\n",
    "        for i in tqdm(range(0,k_fold_num)):\n",
    "            X_train = pd.concat([train_dataset.iloc[:split_ptr[i],:-14],train_dataset.iloc[split_ptr[i+1]:,:-14]],axis=0)\n",
    "            y_train = pd.concat([train_dataset.iloc[:split_ptr[i],-14:],train_dataset.iloc[split_ptr[i+1]:,-14:]],axis=0)\n",
    "            X_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],:-14]\n",
    "            y_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],-14:]\n",
    "            binary_ = Binary_Relevance(base_estimator_list[j],True,False)\n",
    "            binary_.train_fit(X_train,y_train)\n",
    "            y_pred = binary_.predict_model(X_valid)\n",
    "            accuracy_aver1 += metrics.hamming_loss(y_valid, y_pred)\n",
    "            accuracy_aver2 += metrics.f1_score(y_valid, y_pred, average='macro')\n",
    "        accuracy_list1.iloc[1,j]=accuracy_aver1/k_fold_num\n",
    "        accuracy_list2.iloc[1,j]=accuracy_aver2/k_fold_num\n",
    "    return (accuracy_list1,accuracy_list2)\n",
    "    \n",
    "(accuracy_list1,accuracy_list2) = evaluation(dataset,base_estimator_list,accuracy_list1,accuracy_list2,10)\n",
    "print(\"Hamming loss\")\n",
    "print(accuracy_list1)\n",
    "print(\"macro-averaged f-score\")\n",
    "print(accuracy_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class Classifier_Chains():\n",
    "    def __init__(self, BaseEstimator=\"decision tree\",undersampling = False,print_ = True):\n",
    "        \"\"\"Classifier Chains for multi-label classification\n",
    "        Use train_fit to train model, and use predict_model to predict\n",
    "        Parameters\n",
    "        ----------\n",
    "        BaseEstimator : 'decision tree' or 'logistic regression' or 'SVM'.\n",
    "        undersampling : True or False\n",
    "        print_ : print process or not\n",
    "        -------\n",
    "        \"\"\"\n",
    "        self.BaseEstimator = BaseEstimator\n",
    "        self.feature_num = 0\n",
    "        self.undersampling = undersampling\n",
    "        self.print_ = print_\n",
    "        \n",
    "    def get_class_order(self,X,y):\n",
    "        accuracy_n = []\n",
    "        binary_ = Binary_Relevance(self.BaseEstimator,False,False)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,\\\n",
    "                                random_state=0,train_size = 0.7)\n",
    "        binary_.train_fit(X_train,y_train)\n",
    "        y_pred = binary_.predict_model(X_test)\n",
    "        for i in range(0,len(y_test.iloc[0,:])):        \n",
    "            accuracy_ptr =metrics.accuracy_score(y_test.iloc[:,i],y_pred.iloc[:,i])\n",
    "            accuracy_n.append(accuracy_ptr)\n",
    "        accuracy_n=np.array(accuracy_n)\n",
    "        self.class_order = np.argsort(accuracy_n)\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def train_fit(self,X,y):\n",
    "        if self.print_==True:\n",
    "            print(\"Use %s to train\"%self.BaseEstimator)\n",
    "        self.feature_num = len(y.iloc[0,:])\n",
    "        self.get_class_order(X,y)\n",
    "        if self.print_==True:\n",
    "            print(str(\"Class order\"),end=\"\")\n",
    "            print(self.class_order)\n",
    "        for i in self.class_order:\n",
    "            if self.undersampling==False:\n",
    "                y_slice = y.iloc[:,i]\n",
    "                X_slice = X\n",
    "            elif self.undersampling==True:\n",
    "                X_slice,y_slice=RandomUnderSampler(random_state=0).fit_resample(X, y.iloc[:,i])\n",
    "            if self.BaseEstimator == 'SVM':\n",
    "                model = BaggingClassifier(base_estimator=SVC()).fit(X_slice,y_slice)\n",
    "                joblib.dump(model, \"model_store/task4_%s.m\"%(str(i)))\n",
    "            elif self.BaseEstimator == 'logistic regression':\n",
    "                model = BaggingClassifier(base_estimator=LogisticRegression()).fit(X_slice,y_slice)\n",
    "                joblib.dump(model, \"model_store/task4_%s.m\"%(str(i)))\n",
    "            elif self.BaseEstimator == 'decision tree':\n",
    "                model = RandomForestClassifier().fit(X_slice,y_slice)\n",
    "                joblib.dump(model, \"model_store/task4_%s.m\"%(str(i)))\n",
    "            X = pd.concat([X,y.iloc[:,i]],axis = 1)\n",
    "        return self\n",
    "    \n",
    "    def predict_model(self,X):\n",
    "        y_output = pd.DataFrame() \n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        for i in self.class_order:\n",
    "            model = joblib.load(\"model_store/task4_%s.m\"%(str(i)))\n",
    "            y_ptr = pd.DataFrame(model.predict(X))\n",
    "            y_ptr.columns=['Class'+str(i)]\n",
    "            y_output = pd.concat([y_output,y_ptr],axis = 1)\n",
    "            X = pd.concat([X,y_ptr],axis = 1)\n",
    "            \n",
    "        y_output=y_output[['Class'+str(i) for i in range(0,14)]]\n",
    "        if self.print_==True:\n",
    "            print(\"Successfully predict\")\n",
    "        return y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use decision tree to train\n",
      "Class order[ 1  3  2 12 11  4  0  7  5  6 10  9  8 13]\n",
      "Successfully predict\n",
      "Use logistic regression to train\n",
      "Class order[ 1  2 12  3 11  4  0  5  7  6 10  9  8 13]\n",
      "Successfully predict\n",
      "Use SVM to train\n",
      "Class order[ 1 12  3  2 11  0  7  4  5  6 10  9  8 13]\n",
      "Successfully predict\n",
      "         decision tree  logistic regression       SVM\n",
      "Class1        0.986226             0.986226  0.986226\n",
      "Class2        0.720386             0.709366  0.719008\n",
      "Class3        0.727273             0.716253  0.725895\n",
      "Class4        0.891185             0.877410  0.877410\n",
      "Class5        0.896694             0.887052  0.884298\n",
      "Class6        0.920110             0.920110  0.920110\n",
      "Class7        0.814050             0.782369  0.790634\n",
      "Class8        0.849862             0.809917  0.807163\n",
      "Class9        0.786501             0.758953  0.757576\n",
      "Class10       0.790634             0.747934  0.765840\n",
      "Class11       0.747934             0.695592  0.705234\n",
      "Class12       0.691460             0.687328  0.673554\n",
      "Class13       0.632231             0.611570  0.644628\n",
      "Class14       0.763085             0.754821  0.775482\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use Classifier_Chains() to train and predit with imbalanced training data, and\n",
    "show the accuracy of each feature of different BaseEstimator\n",
    "\"\"\"\n",
    "def classifier_chains_train_imbalanced():\n",
    "    base_estimator_list=['decision tree', 'logistic regression', 'SVM']\n",
    "    accuracy_list_for_imbalanced = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['Class'+str(i) for i in range(1,15)])\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        chains_ = Classifier_Chains(base_estimator_list[j],False,True)\n",
    "        chains_.train_fit(X_train,y_train)\n",
    "        y_pred = chains_.predict_model(X_test)\n",
    "        accuracy_n = []\n",
    "        for i in range(0,len(y_test.iloc[0,:])):\n",
    "            accuracy_ptr =metrics.accuracy_score(y_test.iloc[:,i],y_pred.iloc[:,i])\n",
    "            accuracy_n.append(accuracy_ptr)\n",
    "        accuracy_n.reverse()\n",
    "        accuracy_list_for_imbalanced.iloc[:,j]=accuracy_n\n",
    "    return accuracy_list_for_imbalanced\n",
    "accuracy_list_for_imbalanced = classifier_chains_train_imbalanced()\n",
    "print(accuracy_list_for_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use decision tree to train\n",
      "Class order[ 1  3  2 12 11  4  0  5  7  6 10  9  8 13]\n",
      "Successfully predict\n",
      "Use logistic regression to train\n",
      "Class order[ 1  2  3 12 11  4  0  5  7  6 10  9  8 13]\n",
      "Successfully predict\n",
      "Use SVM to train\n",
      "Class order[ 1  2 12  3 11  0  5  4  7  6 10  9  8 13]\n",
      "Successfully predict\n",
      "         decision tree  logistic regression       SVM\n",
      "Class1        0.717631             0.742424  0.772727\n",
      "Class2        0.578512             0.563361  0.589532\n",
      "Class3        0.582645             0.564738  0.590909\n",
      "Class4        0.680441             0.713499  0.679063\n",
      "Class5        0.684573             0.724518  0.687328\n",
      "Class6        0.564738             0.672176  0.559229\n",
      "Class7        0.652893             0.709366  0.690083\n",
      "Class8        0.681818             0.721763  0.706612\n",
      "Class9        0.757576             0.703857  0.701102\n",
      "Class10       0.772727             0.695592  0.699725\n",
      "Class11       0.709366             0.666667  0.741047\n",
      "Class12       0.688705             0.681818  0.703857\n",
      "Class13       0.630854             0.614325  0.650138\n",
      "Class14       0.703857             0.710744  0.774105\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use Classifier_Chains() to train and predit with imbalanced training data, and\n",
    "show the accuracy of each feature of different BaseEstimator\n",
    "\"\"\"\n",
    "def classifier_chains_train_imbalanced():\n",
    "    base_estimator_list=['decision tree', 'logistic regression', 'SVM']\n",
    "    accuracy_list_for_imbalanced = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['Class'+str(i) for i in range(1,15)])\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        chains_ = Classifier_Chains(base_estimator_list[j],True,True)\n",
    "        chains_.train_fit(X_train,y_train)\n",
    "        y_pred = chains_.predict_model(X_test)\n",
    "        accuracy_n = []\n",
    "        for i in range(0,len(y_test.iloc[0,:])):\n",
    "            accuracy_ptr =metrics.accuracy_score(y_test.iloc[:,i],y_pred.iloc[:,i])\n",
    "            accuracy_n.append(accuracy_ptr)\n",
    "        accuracy_n.reverse()\n",
    "        accuracy_list_for_imbalanced.iloc[:,j]=accuracy_n\n",
    "    return accuracy_list_for_imbalanced\n",
    "accuracy_list_for_imbalanced = classifier_chains_train_imbalanced()\n",
    "print(accuracy_list_for_imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating imbalanced data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:07<00:00, 30.71s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:51<00:00,  5.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [06:51<00:00, 41.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating balanced data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:23<00:00, 20.37s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:37<00:00,  3.79s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:52<00:00, 29.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss\n",
      "           decision tree logistic regression       SVM\n",
      "imbalanced      0.191286            0.212943  0.212105\n",
      "balanced        0.324185            0.311368  0.304586\n",
      "macro-averaged f-score\n",
      "           decision tree logistic regression       SVM\n",
      "imbalanced       0.38904            0.383174  0.411222\n",
      "balanced        0.463796            0.427854  0.427047\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate use Hamming loss and macro-averaged f-score with k-fold cross validation.\n",
    "\"\"\"\n",
    "base_estimator_list=['decision tree', 'logistic regression', 'SVM']   \n",
    "\n",
    "accuracy_list1 = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['imbalanced','balanced'])\n",
    "accuracy_list2 = pd.DataFrame(columns=base_estimator_list\\\n",
    "                             ,index=['imbalanced','balanced'])\n",
    "\n",
    "def evaluation(train_dataset,base_estimator_list,accuracy_list1,accuracy_list2,k_fold_num):\n",
    "    def random_split(full_list,k_fold_num1):\n",
    "        offset = int(len(full_list)/k_fold_num1)\n",
    "        full_list = full_list.sample(frac=1).reset_index(drop=True)\n",
    "        split_point = []\n",
    "        for i in range(0,k_fold_num1):\n",
    "            split_point.append(offset*i)\n",
    "        split_point.append(len(full_list))\n",
    "        return split_point\n",
    "    split_ptr = random_split(train_dataset,k_fold_num)\n",
    "    print(\"evaluating imbalanced data\")\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        accuracy_aver1,accuracy_aver2 = 0,0\n",
    "        for i in tqdm(range(0,k_fold_num)):\n",
    "            X_train = pd.concat([train_dataset.iloc[:split_ptr[i],:-14],train_dataset.iloc[split_ptr[i+1]:,:-14]],axis=0)\n",
    "            y_train = pd.concat([train_dataset.iloc[:split_ptr[i],-14:],train_dataset.iloc[split_ptr[i+1]:,-14:]],axis=0)\n",
    "            X_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],:-14]\n",
    "            y_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],-14:]\n",
    "            chain_ = Classifier_Chains(base_estimator_list[j],False,False)\n",
    "            chain_.train_fit(X_train,y_train)\n",
    "            y_pred = chain_.predict_model(X_valid)\n",
    "            accuracy_aver1 += metrics.hamming_loss(y_valid, y_pred)\n",
    "            accuracy_aver2 += metrics.f1_score(y_valid, y_pred, average='macro')\n",
    "        accuracy_list1.iloc[0,j]=accuracy_aver1/k_fold_num\n",
    "        accuracy_list2.iloc[0,j]=accuracy_aver2/k_fold_num  \n",
    "    print(\"evaluating balanced data\")\n",
    "    for j in range(0,len(base_estimator_list)):\n",
    "        accuracy_aver1,accuracy_aver2 = 0,0\n",
    "        for i in tqdm(range(0,k_fold_num)):\n",
    "            X_train = pd.concat([train_dataset.iloc[:split_ptr[i],:-14],train_dataset.iloc[split_ptr[i+1]:,:-14]],axis=0)\n",
    "            y_train = pd.concat([train_dataset.iloc[:split_ptr[i],-14:],train_dataset.iloc[split_ptr[i+1]:,-14:]],axis=0)\n",
    "            X_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],:-14]\n",
    "            y_valid = train_dataset.iloc[split_ptr[i]:split_ptr[i+1],-14:]\n",
    "            chain_ = Classifier_Chains(base_estimator_list[j],True,False)\n",
    "            chain_.train_fit(X_train,y_train)\n",
    "            y_pred = chain_.predict_model(X_valid)\n",
    "            accuracy_aver1 += metrics.hamming_loss(y_valid, y_pred)\n",
    "            accuracy_aver2 += metrics.f1_score(y_valid, y_pred, average='macro')\n",
    "        accuracy_list1.iloc[1,j]=accuracy_aver1/k_fold_num\n",
    "        accuracy_list2.iloc[1,j]=accuracy_aver2/k_fold_num\n",
    "    return (accuracy_list1,accuracy_list2)\n",
    "    \n",
    "(accuracy_list1,accuracy_list2) = evaluation(dataset,base_estimator_list,accuracy_list1,accuracy_list2,10)\n",
    "print(\"Hamming loss\")\n",
    "print(accuracy_list1)\n",
    "print(\"macro-averaged f-score\")\n",
    "print(accuracy_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Hamming loss</center>\n",
    "| Binary Relevance | decision tree | logistic regression | SVM |\n",
    "| :----: | :----:  | :----:  | :----: |\n",
    "| imbalanced | 0.189 | 0.199 | 0.186 |\n",
    "| balanced | 0.347 | 0.365 | 0.341 |\n",
    "\n",
    "| Classifier Chain | decision tree | logistic regression | SVM |\n",
    "|  :----:   | :----:  | :----:  | :----:  |\n",
    "| imbalanced | 0.191 | 0.213 | 0.212 |\n",
    "| balanced | 0.324 | 0.311 | 0.305 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Macro-Averaged F-Score</center>\n",
    "| Binary Relevance | decision tree | logistic regression | SVM |\n",
    "| :----: | :----:  | :----:  | :----: |\n",
    "| imbalanced | 0.365 | 0.349 | 0.383 | \n",
    "| balanced | 0.467 | 0.455 | 0.470 | \n",
    "\n",
    "| Classifier Chain | decision tree | logistic regression | SVM |\n",
    "|  :----:   | :----:  | :----:  | :----:  |\n",
    "| imbalanced | 0.389 | 0.383 | 0.411 |\n",
    "| balanced | 0.464 | 0.428 | 0.427 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/x-mathjax-config\">\n",
    "MathJax.Hub.Config({\n",
    "tex2jax: {\n",
    "inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n",
    "processEscapes: true},\n",
    "jax: [\"input/TeX\",\"input/MathML\",\"input/AsciiMath\",\"output/CommonHTML\"],\n",
    "extensions: [\"tex2jax.js\",\"mml2jax.js\",\"asciimath2jax.js\",\"MathMenu.js\",\"MathZoom.js\",\"AssistiveMML.js\", \"[Contrib]/a11y/accessibility-menu.js\"],\n",
    "TeX: {\n",
    "extensions: [\"AMSmath.js\",\"AMSsymbols.js\",\"noErrors.js\",\"noUndefined.js\"],\n",
    "equationNumbers: {\n",
    "autoNumber: \"AMS\"\n",
    "}\n",
    "}\n",
    "});\n",
    "</script>\n",
    "\n",
    "#### Evaluation results:    \n",
    "1. The performance of imbalanced data is better than balanced data, because undersampling delete part of the origin train dataset to balance data.  \n",
    "2. Classifier Chain achieves better performance than Binary Relevance with balanced data while perform a little worse with biased data.  \n",
    "\n",
    "#### Complexity Analysis:    \n",
    "1. The computational complexity of binary relevance is $\\mathcal{O}(L\\times f(d,N))$,while the complexity of classifier chain is $\\mathcal{O}(L\\times f(d+L,N))$, where L denotes the number of functional classes, $f(d,N)$ denotes the complexity of base estimator with $N$ examples and $d$ attributes.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
